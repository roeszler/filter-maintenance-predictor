{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI Portfolio Project 5 - Filter Maintenance Predictor 2022\n",
    "## **Feature Engineering Notebook**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Cleaning**\n",
    "\n",
    "Performed within the [data cleaning notebook](https://github.com/roeszler/filter-maintenance-predictor/blob/main/jupyter_notebooks/02_DataCleaning.ipynb).\n",
    "\n",
    "**2. Data Transformation**\n",
    "* Processing the data for the modelling stage.\n",
    "* Transform data into a format that is useful for the algorithm learn the relationship among the variables.\n",
    "* Evaluate the use of the following approaches to engineer the variables:\n",
    "    * ordinal categorical encoding\n",
    "    * numerical transformation\n",
    "    * smart correlated selection\n",
    "    \n",
    "**3. Feature Extraction**\n",
    "* Evenly distribute dust type\n",
    "\n",
    "**4. Feature Selection**\n",
    "\n",
    "**5. Feature Iteration**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inputs\n",
    "\n",
    "1. Cleaned Test Dataset : `outputs/datasets/collection/dfCleanTrain.csv`\n",
    "\n",
    "2. Cleaned Train Dataset : `outputs/datasets/collection/dfCleanTrain.csv`\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* Generate engineered Train and Test sets, both saved under `outputs/datasets/transformed`\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "  * Best approach to engineer variables based on...\n",
    "  * Transformations that we will consider in our pipeline are..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"Current directory set to new location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_total = pd.read_csv(f'outputs/datasets/cleaned/dfCleanTotal.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "## Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to define\n",
    "* Feature Extraction: **creates** new features from functions of the original features.\n",
    "* Feature Selection: allows us to **choose** a subset of the features for use in model construction.\n",
    "\n",
    "Semantically we could argue each are a **new feature** or a **subset** of the feature. We have however considered any value intended for use in model construction as a **Selected Feature**: *The process of selecting a subset of relevant features (variables, predictors) for use in model construction*. \n",
    "\n",
    "Feature selection techniques are used for several reasons:\n",
    "\n",
    "* Simplification of models to make them easier to interpret by researchers/users\n",
    "* Shorter training times\n",
    "* Avoiding too many input variables (dimensionality)\n",
    "* Improve the data's compatibility with a learning model class\n",
    "* Create symmetries in the input data.\n",
    "\n",
    "The main idea when using a feature selection is that the data contains some features that are either **surplus**, **redundant** or **irrelevant** to the final business goal and can therefore be removed without incurring much loss of predictive power."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Categorical encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing categories with ordinal numbers \n",
    "The dataset as two categorical features, **Data_No** and **Dust**.\n",
    "\n",
    "These have been been transformed in the **01_DataCollection** notebook to assist us with the manipulation of the data.  They are in the format we desire at this point. We may need to transform these back into their respective categorical values, however this won't be performed until required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_no_total = df_total['Data_No'].map(str)\n",
    "# df_total['Data_No'] = data_no_total\n",
    "# df_total.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical transformations\n",
    "\n",
    "This process can consider transformers like:\n",
    "* Smooth with SMA\n",
    "* Logarithmic in base e\n",
    "* Logarithmic in base 10\n",
    "* Reciprocal\n",
    "* Power\n",
    "* BoxCox\n",
    "* Yeo Johnson\n",
    "\n",
    "The dataset has several numerical features to assess. A [custom function](https://github.com/Code-Institute-Solutions/churnometer/blob/main/jupyter_notebooks/04%20-%20FeatureEngineering.ipynb) found at the [code institute] has been included as a quick way to represent the data for evaluation. Including the full report into this notebook made it a little unwieldy. It has been included as an addendum notebook at [04_FeatEngAddendum.ipynb](https://github.com/roeszler/filter-maintenance-predictor/blob/main/jupyter_notebooks/03A_FeatEngAddendum.ipynb) optionally for your review. \n",
    "\n",
    "**In summary**, it reveals:\n",
    "* BoxCox and Yeo Johnson for most variables improve `Time` and `Differential Pressure` features, however the existence of negative **differential pressure** values present a challenge to log transform this data.\n",
    "* Due to the [value of log transformation](https://stats.stackexchange.com/questions/107610/what-is-the-reason-the-log-transformation-is-used-with-right-skewed-distribution) to regression models, the following notebook follows a modification and filtering of the data to remove negative values and apply a log transformation. \n",
    "* Notwithstanding, a **Modified Log Transformed**, **BoxCox** and **Yeo Johnson** will be considered at each modelling stage to evaluate their effects on model performance. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change in Differential Pressure\n",
    "Include change in Differential Pressure calculation\n",
    "\n",
    "**Note**: We replace first instance of `change_DP` with first value of `differential_pressure` in the new bin. \n",
    "* This signifies that the fist observation starts from a **zero** DP value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_change_dp = pd.DataFrame()\n",
    "\n",
    "list_data_nos = list(df_total['Data_No'].unique())\n",
    "for n in list_data_nos:\n",
    "    if (df_total.Data_No != df_total.Data_No.shift(1)).any().any():\n",
    "        df_bin = df_total[df_total['Data_No'] == n]\n",
    "\n",
    "        change_dp_calc = df_bin['Differential_pressure'].diff().fillna(df_bin['Differential_pressure'])\n",
    "        df_bin.insert(loc=7, column='change_DP', value=change_dp_calc)\n",
    "\n",
    "        df_change_dp = pd.concat([df_change_dp, df_bin], ignore_index = True)\n",
    "df_total = df_change_dp\n",
    "df_total.loc[446:451]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Part of the challenge with this data is dealing with a continuous dataset that is comprised of data 'bins'. Each `Data_No` data bin represents an individual test cycle. \n",
    "\n",
    "When engineering **descriptive statistics** into the dataset, that typically used a sample of the previous values to indicate a mean or standard deviation, **we need to treat each bin individually** and not progress the first row of each bin incorrectly with the last rows of the previous bin.\n",
    "\n",
    "To solve this, we use a sequential loop that proceeds through each bin and inserts a progressive descriptive statistic calculation like change in differential pressure (`change_DP`) and appends it to the previous bin. This eventually creates a version of the dataset that includes the new calculation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers in differential pressure observations\n",
    "\n",
    "In each bin we notice that the size and direction of the `change_DP` measure occasionally produces a zero or negative value. This highlights a fluctuation in the **differential pressures**. These may be considered outliers as the pressure gradient across the filter needs time to stabilize. We have considered three main methods to deal with these observations:\n",
    "* Log transformation\n",
    "* Winsorize method\n",
    "* Dropping the outliers\n",
    "\n",
    "These will be handled in the [feature engineering](https://github.com/roeszler/filter-maintenance-predictor/blob/main/jupyter_notebooks/03_FeatureEngineering.ipynb) notebook\n",
    "\n",
    "**Random sample** (Bin 96) to plot and inspect change in Differential Pressure distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,5))\n",
    "\n",
    "bin_96 = df_total[df_total['Data_No'] == 96]\n",
    "sns.boxplot(x = bin_96['change_DP'], ax=ax1)\n",
    "sns.histplot(x = bin_96['change_DP'], ax=ax2)\n",
    "sns.lineplot(x=bin_96['Time'], y=bin_96['Differential_pressure'], ax=ax3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing of **Differential Pressure**\n",
    "*Dealing with outliers in differential pressure observations*\n",
    "\n",
    "In each bin note `differential_pressure` and `change_DP` observations. Occasionally the measures fluctuate outside of the general trend of the data (outliers). This can be seen below in the **second to last** observation of Data_No bin 98 (index no. 75826) where the recorded value is outside the general trend of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total['Data_No'] == 98].tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- To quantify such measures, have considered a tolerance of ±200% change in value, however this can be altered depending on what we see when fitting the models. -->\n",
    "\n",
    "To **smooth** the variability of this measure we can apply a **mean** (or average) value to the data in various ways. \n",
    "This attempts to soften the severity of changes seen and reduce the instances of values that are higher / lower than the general trend. It will effectively reduce variability in the **differential pressure** measure, making it easier to model.\n",
    "\n",
    "We have considered the following methods to deal with outliers:\n",
    "* Smooth with Simple Mean Average (SMA)\n",
    "* Exponentially Weighted Mean (EWM)\n",
    "* Log transformation\n",
    "* Dropping the outliers\n",
    "* Winsorize method\n",
    "\n",
    "For each calculation that uses previous observations to produce a value, we can **use the same process we applied to manage the unique data bins** as we did in calculating change in differential pressure above.\n",
    "\n",
    "#### Smoothing of Differential Pressure with a **Simple Moving Average** (SMA)\n",
    "A simple moving average (SMA) is an arithmetic **moving average** calculated by adding recent values (the last four measures in this case), then dividing that by the number of observations in the calculation. This moves along as the observations progresses.\n",
    "\n",
    "* We can see the first four observations are **NaN** indicated. \n",
    "    * These could be imputed with arbitrary values, mean values, closest k sample values and/or a MICE (Multiple Imputation by Chained Equations) value that fits a linear regression with the present values.\n",
    "    * On review, the MICE method would be the preferred method to impute the missing SMA values, **however**: considering the progressive nature of the `differential_pressure` variable, a preferable alternate to SMA would be Exponentially Weighted Mean (EWM).\n",
    "\n",
    "#### Smoothing of Differential Pressure with an **Exponentially Weighted Mean** (EWM)\n",
    "Is a measure of the moving average that considers older observations to have given lower weightings in the calculation. The weights fall exponentially as the data point gets older – hence the name exponentially weighted.\n",
    "\n",
    "* The EWM is calculated with a 4 point EWM (`span=4`) measure, that considers the previous 4 observations to calculate the weighted mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means = pd.DataFrame()\n",
    "\n",
    "list_data_nos = list(df_total['Data_No'].unique())\n",
    "for n in list_data_nos:\n",
    "    if (df_total.Data_No != df_total.Data_No.shift(1)).any().any():\n",
    "        df_bin = df_total[df_total['Data_No'] == n]\n",
    "\n",
    "        ewm_calc = df_bin['Differential_pressure'].ewm(span=4, adjust=False).mean()\n",
    "        df_bin.insert(loc=2, column='4point_EWM', value=ewm_calc)\n",
    "\n",
    "        sma_calc = df_bin['Differential_pressure'].rolling(4).mean()\n",
    "        df_bin.insert(loc=2, column='4point_SMA', value=sma_calc)\n",
    "\n",
    "        change_ewm_calc = df_bin['4point_EWM'].diff().fillna(df_bin['4point_EWM'])\n",
    "        df_bin.insert(loc=10, column='change_EWM', value=change_ewm_calc)\n",
    "\n",
    "        df_means = pd.concat([df_means, df_bin], ignore_index = True)\n",
    "df_total = df_means\n",
    "df_total.loc[446:451]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logarithmic Transformation** of Differential Pressure\n",
    "\n",
    "As seen at data collection, the original continuous `differential_pressure` data is right or **positively skewed** at 1.81 and does not follow the shape of a normally distributed bell curve.\n",
    "* Included a plot for both instances of the **moving average smoothed** variable to check that it is representative of the **differential_pressure** source, which it is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,6))\n",
    "\n",
    "sns.histplot(x = df_total['Differential_pressure'], ax=ax1)\n",
    "sns.histplot(x = df_total['4point_SMA'], ax=ax2)\n",
    "sns.histplot(x = df_total['4point_EWM'], ax=ax3)\n",
    "\n",
    "ax1.title.set_text('Differential Pressure Histogram\\n')\n",
    "ax2.title.set_text('Simple Moving Average Histogram\\n')\n",
    "ax3.title.set_text('Exponential Weighted Mean Histogram\\n')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **log transformation** of this data will represent the values within a normal distribution, **as much as possible**, allowing a more valid statistical analysis from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,6))\n",
    "\n",
    "df_total_log_dp = df_total['Differential_pressure']\n",
    "log_dp = np.log(df_total_log_dp)\n",
    "sns.histplot(x = log_dp, ax=ax1)\n",
    "\n",
    "df_total_log_sma = df_total['4point_SMA']\n",
    "log_sma = np.log(df_total_log_sma)\n",
    "sns.histplot(x = log_sma, ax=ax2)\n",
    "\n",
    "df_total_log_ewm = df_total['4point_EWM']\n",
    "log_ewm = np.log(df_total_log_ewm)\n",
    "sns.histplot(x = log_ewm, ax=ax3)\n",
    "\n",
    "# plt.title('Log Histogram')\n",
    "ax1.title.set_text('Log_DP Histogram\\n')\n",
    "ax2.title.set_text('Log_SMA Histogram\\n')\n",
    "ax3.title.set_text('Log_EWM Histogram\\n')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** \n",
    "* The shape of the numerical differential pressure data is **improved** by a natural logarithmic transformation.\n",
    "    <!-- * Untreated values of 85.71 mean, std.deviation at 114.34 and median at 38.34, indicates the positively skewed nature of the original data\n",
    "    * Transformed the data is much improved with mean 3.18 much closer to the median 3.89 -->\n",
    "\n",
    "The transformed data is still affected by the negative skew in the data:\n",
    "* Pressure measures show values at zero or below at the beginning and tail of the data. This is amplified when we transform the mean data sets.\n",
    "    * The instances of negative numbers can be managed by:\n",
    "        * Adding a constant to stop the value becoming negative, or \n",
    "        * Indicate the negative as a missing number or \n",
    "        * Dropping the entire observation\n",
    "    * Considering these measures tend to fall in zones at the beginning or end of the test where the procedure takes a while to 'equalize' or has passed the point of filter failure, we have decided to treat them as outliers and confidently drop the entire row these observation sits within without impacting the power of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "# df.insert(loc=0, column='log_EWM', value=(log_ewm*10))\n",
    "# df.insert(loc=0, column='log_SMA', value=(log_sma*10))\n",
    "# df.insert(loc=0, column='log_DP', value=(log_dp*10))\n",
    "# df.insert(loc=0, column='DP', value=df_total_log_dp)\n",
    "# round(df.describe(), 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further EWM transformation**\n",
    "\n",
    "Remove rows with negative numbers in `4pointWEM` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_total.insert(loc=4, column='log_EWM', value=log_ewm)\n",
    "# data = df_total.loc[:, df_total.columns == 'log_EWM']\n",
    "# df_total = df_total[data.select_dtypes(include=[np.number]).ge(-0).all(1)]\n",
    "# # df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Negative values\n",
    "df_total.insert(loc=4, column='log_EWM', value=log_ewm)\n",
    "data = df_total.loc[:, df_total.columns == 'log_EWM']\n",
    "df_total = df_total[data.select_dtypes(include=[np.number]).ge(-0).all(1)]\n",
    "\n",
    "# Visualize the data\n",
    "old_shape = pd.read_csv(f'outputs/datasets/cleaned/dfCleanTotal.csv')\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "sns.histplot(x = log_dp, ax=ax1)\n",
    "sns.histplot(x = df_total['log_EWM'], ax=ax2)\n",
    "\n",
    "plt.title('log_EWM Histogram')\n",
    "ax1.title.set_text('Transformed Differential Pressure Histogram\\n')\n",
    "ax2.title.set_text('Transformed Differential Pressure Exponentially Weighted Mean Histogram (Values > 0)\\n')\n",
    "plt.show()\n",
    "print(\"Old Shape: \", old_shape.shape)\n",
    "print(\"New Shape: \", df_total.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformed data is far more useable to train the model. \n",
    "* Less variability \n",
    "* Negative values removed\n",
    "* Minimal loss of data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Management of Outliers\n",
    "Dropping all negative numbers to reduce the noise of the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of detecting outliers using **Inter Quartile Range** (IQR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_IQR = df_total.copy()\n",
    "\n",
    "# IQR\n",
    "Q1 = np.percentile(df_total_IQR, 25, method='midpoint')\n",
    "Q3 = np.percentile(df_total_IQR, 75,method='midpoint')\n",
    "IQR = Q3 - Q1\n",
    "print(\"Old Shape: \", df_total_IQR.shape)\n",
    " \n",
    "# Upper bound\n",
    "upper = np.where(df_total_IQR >= (Q3+1.5*IQR))\n",
    "# Lower bound\n",
    "lower = np.where(df_total_IQR <= (Q1-1.5*IQR))\n",
    " \n",
    "''' Removing the Outliers '''\n",
    "df_total_IQR.drop(upper[0], inplace = True)\n",
    "df_total_IQR.drop(lower[0], inplace = True)\n",
    " \n",
    "print(\"New Shape: \", df_total_IQR.shape)\n",
    "\n",
    "# df_total_log_IQR = df_total_IQR['Differential_pressure']\n",
    "# df_total_log_IQR = df_total_IQR['4point_SMA']\n",
    "# df_total_log_IQR = df_total_IQR['4point_EWM']\n",
    "df_total_log_IQR = df_total_IQR['log_EWM']\n",
    "log_dp_IQR = np.log(df_total_log_IQR)\n",
    "# df_total_IQR['log_DP'] = log_dp\n",
    "sns.histplot(x = df_total_log_IQR)\n",
    "plt.title('IQR Histogram of log EWM')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR transformation is not particularly effective on this data with negative values removed. We will not apply this method and proceed with other techniques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The Winsorize Method**\n",
    "Winsorization is the process of replacing the extreme values of statistical data in order to limit the effect of the outliers on the calculations or the results obtained by using that data. To apply this to a our exponentially logged differential pressure `log_EWM` variable, where outliers are present only at one end of the data:\n",
    "* The lower 10% values of the data will have their values set equal to the value of the data point at the 10th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "WinsorizedArrayMean = np.mean(df_total['log_EWM'])\n",
    "winz_EWM = winsorize(df_total['log_EWM'],(0.1,0.1))\n",
    "df_total.insert(loc=5, column='winz_EWM', value=winz_EWM)\n",
    "WinsorizedArray = df_total['winz_EWM']\n",
    "plt.boxplot(WinsorizedArray)\n",
    "plt.title('Winsorized array')\n",
    "plt.show()\n",
    "WinsorizedArrayNewMean = np.mean(WinsorizedArray)\n",
    "print('Old Mean: ', WinsorizedArrayMean)\n",
    "print('New Mean: ', WinsorizedArrayNewMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_log_winz = df_total['winz_EWM']\n",
    "log_winz = np.log(df_total_log_winz)\n",
    "sns.histplot(x = log_winz)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winzorization smooths the data too much and does not add much value to training our models so we will not apply this method either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_total['winz_EWM']\n",
    "del df_total['4point_SMA']\n",
    "# del df_train['winz_Mean']\n",
    "# del df_train['log_DP']\n",
    "df_total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evenly distribute dataset by `Dust` type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the **train** and **test** sets supplied have data distributed unevenly between 50 test bins. To account for this we wish to assess the measures of central tendency for each Dust class, with the aim of reducing the data size to a more evenly proportioned one between classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train** Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considerations**\n",
    "\n",
    "* The proportion of data that **has reached filter failure** is represented by how close `filter_balance` is to zero or less. \n",
    "    * Data with filter_balance values approaching zero may be worth keeping and will make part of our heuristic decision process.\n",
    "* Notwithstanding that the **mean** is the most frequently used measure of central tendency because it uses all values in the data set to give you an average\n",
    "    * For data from skewed distributions (like `differential_pressure`), the **median** is better than the mean because it isn’t influenced by extremely large values.\n",
    "\n",
    "In the following calculation, we see a summary of the top ten `Data_No` bins where `differential_pressure` observations that have made it to the **600 Pa** (the point of filter failure)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide back into **Train** and **Test** sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_total = df_total['Data_No'].map(int).round(decimals=0)\n",
    "df_total.loc[:, 'Data_No'] = data_no_total\n",
    "\n",
    "n = df_total['Data_No'].iloc[0:len(df_total)]\n",
    "df_train = df_total[n < 51].reset_index(drop=True, names='index')\n",
    "df_test = df_total[n > 50].reset_index(drop=True, names='index')\n",
    "del df_train['RUL']\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_row_train = df_train[df_train.Data_No != df_train.Data_No.shift(-1)]\n",
    "# last_row_descending = last_row_train.sort_values(by='Dust', ascending=True)\n",
    "last_row_descending = last_row_train.sort_values(by='Differential_pressure', ascending=False)\n",
    "last_row_descending.head(n=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the diagram below showing proportions of `Dust` variable in the **df_train** dataset.\n",
    "* It shows a disproportionate mix between classes. This will be the first dataset we tidy up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "category_totals = df_train.groupby('Dust')['Differential_pressure'].count().sort_values()\n",
    "category_totals.plot(kind=\"barh\", title='Proportion of Dust Classes in df_train\\n', xlabel='\\nObservations', ylabel='Dust Class')\n",
    "category_totals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our next aim is to \n",
    "* Fill these bins with data that best represents a central tendency.\n",
    "* Make the size of each bin around ±**6100** observations (similar to the A4 Coarse Dust class bin) \n",
    "\n",
    "#### Procedure\n",
    "* Include a comparison to how far each `differential_pressure` measure **deviates** or how far it is from the **.median()** value of the bin.\n",
    "* Ordered by `filter_balance` showing sets with data closest to 600 Pa `differential_pressure`.\n",
    "* Include comparison to median\n",
    "* Add a cumulative measure of Data_No's to use as a ranking\n",
    "* Create a dataframe of the A3 Medium Dust : **1.025**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a calculation of Standard Deviation to **df_train** test set and hide several colums for easier viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df_train['winz_Mean']\n",
    "# del df_train['log_DP']\n",
    "std_group = df_train.groupby('Data_No').std()\n",
    "std_group.index.name = None\n",
    "std_group.loc[:,'Data_No'] = std_group.index\n",
    "map_std = df_train['Data_No'].map(std_group.set_index('Data_No')['Differential_pressure'])\n",
    "df_train.loc[:,'std_DP'] = map_std\n",
    "# df_test.loc[363:368]\n",
    "df_train.loc[30082:30087].style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a calculation of Coefficient of Variation (variance) to **df_train** test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cv = lambda data: np.std(data, ddof=1) / np.mean(data, axis=0) * 100 \n",
    "var_group = df_train.groupby('Data_No').apply(cv)\n",
    "var_group.index.name = None\n",
    "var_group.loc[:,'Data_No'] = var_group.index\n",
    "map_var = df_train['Data_No'].map(var_group.set_index('Data_No')['Differential_pressure'])\n",
    "df_train.loc[:,'cv_DP'] = map_var\n",
    "# df_test.loc[363:368]\n",
    "df_train.loc[444:453]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of variation is an indication of how far the standard deviation is away from the mean. As we can see it does not add value to our understanding of the data, primarily due the the skewed nature of the `differential_pressure` continuous variable. \n",
    "* This re-enforces the understanding that descriptive statistics using the mean may not be preferred measure of central tendency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Coefficient of Variation and Add Median to df_train**\n",
    "* Median is the preferred measure of central tendency to observe in a skewed dataset such as this as it is not as affected by larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train['cv_DP']\n",
    "median_group = df_train.groupby('Data_No').median()\n",
    "median_group.index.name = None\n",
    "median_group.loc[:,'Data_No'] = median_group.index\n",
    "map_median = df_train['Data_No'].map(median_group.set_index('Data_No')['Differential_pressure'])\n",
    "df_train.loc[:,'median_DP'] = map_median\n",
    "# df_train.loc[444:453]\n",
    "df_train.loc[444:453].head().style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can evaluate the dataframe with just **A3 Dust** in it, ordered by `filter_balance` as a measure of time to filter failure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the size of each bin and include a cumulative sum of each **bin size** to help see which data bin that reaches **6100** or more total values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_sum = df_train.groupby('Data_No')['Data_No'].count().reset_index(name='bin_tot')\n",
    "map_bin = df_train['Data_No'].map(bin_sum.set_index('Data_No')['bin_tot'])\n",
    "df_train.loc[:, 'bin_size'] = map_bin\n",
    "\n",
    "dust_A3 = df_train[df_train['Dust'] == 1.025]\n",
    "filter_A3 = dust_A3[dust_A3.Data_No != dust_A3.Data_No.shift(-1)]\n",
    "df_train_A3 = filter_A3.sort_values(by='filter_balance', ascending=True)\n",
    "\n",
    "df_train_A3['c_sum'] = df_train_A3['bin_size'].cumsum()\n",
    "dn_fb = df_train_A3.loc[:, 'Data_No'].head(14).sort_values(ascending=True).reset_index(drop=True)\n",
    "df_train_A3.head(14).style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the current dataframe containing only A3 Medium Dust observations, that is ordered by those tests with closest to a completed test to failure:\n",
    "* **The top 9 data bins (seen at bin 7) would extract a A3 Medium dust training dataset with 7,208 observations**\n",
    "* We will now perform a further PDA to evaluate the suitability of these further"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank by Standard Deviations, ordered by `std_DP`\n",
    "The standard deviation is used to measure the spread of values in a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dust_A3 = df_train[df_train['Dust'] == 1.025]\n",
    "# filter_A3 = dust_A3[dust_A3.Data_No != dust_A3.Data_No.shift(-1)]\n",
    "df_train_A3_std = filter_A3.sort_values(by='std_DP', ascending=True)\n",
    "df_train_A3_std['c_sum'] = df_train_A3_std['bin_size'].cumsum()\n",
    "dn_sdv = df_train_A3_std.loc[:, 'Data_No'].head(14).sort_values(ascending=True).reset_index(drop=True)\n",
    "df_train_A3_std.head(14).style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank by central tenancy of the Median value, ordered by `median_DP`\n",
    "* Median = the value of the number in the middle of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dust_A3 = df_train[df_train['Dust'] == 1.025]\n",
    "# filter_A3 = dust_A3[dust_A3.Data_No != dust_A3.Data_No.shift(-1)]\n",
    "df_train_A3_median = filter_A3.sort_values(by='median_DP', ascending=False)\n",
    "df_train_A3_median['c_sum'] = df_train_A3_median['bin_size'].cumsum()\n",
    "dn_mdp = df_train_A3_median.loc[:, 'Data_No'].head(14).sort_values(ascending=True).reset_index(drop=True)\n",
    "df_train_A3_median.head(14).style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review of the **data bin numbers** across descriptors of central tendency that would capture **6100** or more total values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_review = pd.DataFrame()\n",
    "dn_review.insert(loc=0, column='by_std_DP', value=dn_sdv)\n",
    "dn_review.insert(loc=0, column='by_median_DP', value=dn_mdp)\n",
    "dn_review.insert(loc=0, column='by_filter_balance', value=dn_fb)\n",
    "dn_review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations\n",
    "\n",
    "* We can see that when we filter by **filter balance** or **median differential pressure** that most bin numbers are contained in both. In fact only bins 1, 16, and 24 are not contained in both, se we can consider these as the first to re-introduce to the data, perhaps when creating the validation set. \n",
    "    * This provides us with confidence that these test are higher value for our purposes and promote inclusion into our A3 Dust group.\n",
    "* Reviewing those values with the lowest **Standard Deviation** inadvertently includes selects values furthest from fully completing a test (ie differential pressure reaching > 600 Pa). In such, these bins will not factor highly in our considerations to include in the dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "* A **Selected Feature**: *The process of selecting a subset of relevant features (variables, predictors) for use in model construction*. \n",
    "\n",
    "Feature selection techniques are used for several reasons:\n",
    "\n",
    "* Simplification of models to make them easier to interpret by researchers/users\n",
    "* Shorter training times\n",
    "* Avoiding too many input variables (dimensionality)\n",
    "* Improve the data's compatibility with a learning model class\n",
    "* Create symmetries in the input data.\n",
    "\n",
    "The main idea when using a feature selection is that the data contains some features that are either **surplus**, **redundant** or **irrelevant** to the final business goal and can therefore be removed without incurring much loss of predictive power."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting bins from the **Training** dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a separate frame indicating the bin numbers we wish to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_no = df_train_A3['Data_No'].head(9)\n",
    "bin_no.to_frame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these references to create a dataframe `df_train_cleaned_A3` that is ready for inclusion in our final dataframe `df_train_clean`.\n",
    "* Note we disregard the cumulative sum measure as it doesn't add value to further calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train\n",
    "df_train_cleaned_A3 = df_train_copy[df_train_copy['Data_No'].isin(bin_no)]\n",
    "df_train_cleaned_A3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Quick Review: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape started with: ', dust_A3.shape)\n",
    "print('Shape we have now: ', df_train_cleaned_A3.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add these to a new dataset to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_A2 = df_train[df_train['Dust'] == 0.900]\n",
    "dust_A3 = df_train_cleaned_A3\n",
    "dust_A4 = df_train[df_train['Dust'] == 1.200]\n",
    "\n",
    "df_train_compare = pd.concat([dust_A2, dust_A3, dust_A4], ignore_index = True)\n",
    "df_train_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "category_totals = df_train_compare.groupby('Dust')['Differential_pressure'].count().sort_values()\n",
    "category_totals.plot(kind=\"barh\", title='Proportion of Dust Classes in \"df_train_compare\"\\n', xlabel='\\nObservations', ylabel='Dust Class')\n",
    "category_totals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat procedure with remaining `Dust` classes and **Test** dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract, Clear and Replace..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_sum = df_train.groupby('Data_No')['Data_No'].count().reset_index(name='bin_Tot')\n",
    "# map_bin = df_train['Data_No'].map(bin_sum.set_index('Data_No')['bin_Tot'])\n",
    "# df_train.loc[:, 'bin_Size'] = map_bin\n",
    "\n",
    "# df_train.loc[38817:38827]\n",
    "\n",
    "dust_A2 = df_train[df_train['Dust'] == 0.900]\n",
    "filter_A2 = dust_A2[dust_A2.Data_No != dust_A2.Data_No.shift(-1)]\n",
    "df_train_A2 = filter_A2.sort_values(by='filter_balance', ascending=True)\n",
    "df_train_A2['c_sum'] = df_train_A2['bin_size'].cumsum()\n",
    "df_train_A2.head(13).style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_no = df_train_A2['Data_No'].head(9)\n",
    "bin_no.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train\n",
    "df_train_cleaned_A2 = df_train_copy[df_train_copy['Data_No'].isin(bin_no)]\n",
    "df_train_cleaned_A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_A2 = df_train_cleaned_A2\n",
    "dust_A3 = df_train_cleaned_A3\n",
    "dust_A4 = df_train[df_train['Dust'] == 1.200]\n",
    "\n",
    "df_train_compare = pd.concat([dust_A2, dust_A3, dust_A4], ignore_index = True)\n",
    "df_train_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "category_totals = df_train_compare.groupby('Dust')['Differential_pressure'].count().sort_values()\n",
    "category_totals.plot(kind=\"barh\", title='Proportion of Dust Classes in \"df_train_compare\"\\n', xlabel='\\nObservations', ylabel='Dust Class')\n",
    "category_totals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How much data do we need**?\n",
    "* At a bin sample of 9 for both A2 and A3 dust, we currently have a little under 15,000 observations to train the ML Models\n",
    "* Will review this depending on the performance of the models\n",
    "* Overfitting with too much data vs underfitting with too little is a balance and the qualitative nature of the heuristics surrounding the predictions made\n",
    "* Nonlinear Algorithms (like clustering) may need more data\n",
    "\n",
    "Short answer is, **we have plenty of extra data at this point that may be less suited to training the model**. \n",
    "\n",
    "* Notwithstanding, it is still live data and may there may be some value toward increasing the power of our ML Models. \n",
    "* We also have the option to alter the selected number of dust class bins and possibly augmenting the smaller A4 dust dataset with the **Synthetic Minority Oversampling Technique (SMOTE)** or SMOTE NC for categorical data should we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_cleaned = df_train_compare\n",
    "# df_train_cleaned\n",
    "df_train_transformed = df_train_compare\n",
    "df_train_transformed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could run the **test data** through the above process, however it is intended to *test* the models and make sure they’ll work when new data is introduced. We do not want to taylor this data too much as it may promote overfitting the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Feature Selection Considerations\n",
    "\n",
    "Popular filter metrics for **classification problems** are \n",
    "* **Correlation-based feature selection**\n",
    "* Mutual information\n",
    "* Class separability\n",
    "    * Error probability\n",
    "    * Inter-class distance\n",
    "    * Probabilistic distance\n",
    "    * Entropy\n",
    "* **Consistency-based feature selection**\n",
    "* **Correlation-based feature selection**\n",
    "\n",
    "#### Correlation Based Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_transformed.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot relevant relationships to **train** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_train_corr = df_train_transformed.drop(['Data_No', 'Flow_rate', 'Dust', 'mass_g', 'Dust_feed', 'Tt', 'bin_size', 'median_DP', 'std_DP', 'change_DP', 'change_EWM'], axis=\"columns\")\n",
    "sns.pairplot(data=df_train_corr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot relevant relationships to **test** dataset (includes RUL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_corr = df_test.drop(['Data_No', 'Flow_rate', 'Dust', 'mass_g', 'Dust_feed', 'Tt', 'change_DP', 'change_EWM'], axis=\"columns\")\n",
    "sns.pairplot(data=df_test_corr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a quick look, we notice the same relationships between `differential_pressure`, `RUL` and `Time`, with our engineered variables also indicating influence.\n",
    "\n",
    "#### Heatmaps for **df_train** engineered dataset\n",
    "\n",
    "Import ppscore function (Code Institute [Exploratory Data Analysis Tools](https://learn.codeinstitute.net/courses/course-v1:CodeInstitute+DDA101+2021_T4/courseware/468437859a944f7d81a34234957d825b/c8ea2343476c48739676b7f03ba9b08e/) 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ppscore as pps\n",
    "\n",
    "def heatmap_corr(df, threshold, figsize=(10, 8), font_annot=8):\n",
    "    \"\"\"\n",
    "    Heatmap for pearson (linear) and spearman (monotonic) correlations to \n",
    "    visualize only those correlation levels greater than a given threshold.\n",
    "    \"\"\"\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        mask[abs(df) < threshold] = True\n",
    "\n",
    "        fig, axes = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='viridis', annot_kws={'size': font_annot}, ax=axes,\n",
    "                    linewidth=0.01, linecolor='WhiteSmoke'\n",
    "                    )\n",
    "        axes.set_yticklabels(df.columns, rotation=0)\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(10, 8), font_annot=8):\n",
    "    \"\"\"\n",
    "    Heatmap for power predictive score\n",
    "    PPS == 0 means that there is no predictive power\n",
    "    PPS < 0.2 often means that there is some relevant predictive power but it is weak\n",
    "    PPS > 0.2 often means that there is strong predictive power\n",
    "    PPS > 0.8 often means that there is a deterministic relationship in the data,\n",
    "    \"\"\"\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[abs(df) < threshold] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                         mask=mask, cmap='rocket_r', annot_kws={'size': font_annot},\n",
    "                         linewidth=0.01, linecolor='WhiteSmoke')\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def calculate_corr_and_pps(df):\n",
    "    \"\"\"\n",
    "    Calculate the correlations and ppscore of a given dataframe\n",
    "    \"\"\"\n",
    "    df_corr_spearman = df.corr(method='spearman')\n",
    "    df_corr_pearson = df.corr(method='pearson')\n",
    "\n",
    "    pps_matrix_raw = pps.matrix(df)\n",
    "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "    pps_score_stats = pps_matrix_raw.query('ppscore < 1').filter(['ppscore']).describe().T\n",
    "    print('PPS threshold - check PPS score IQR to decide threshold for heatmap \\n')\n",
    "    print(pps_score_stats.round(4))\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "\n",
    "def display_corr_and_pps(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
    "                      figsize=(10, 8), font_annot=8):\n",
    "    \"\"\"\n",
    "    Render the correlations and ppscore heatmaps for a given dataframe\n",
    "    \"\"\"\n",
    "    # print('\\n')\n",
    "    print('To analyze: \\n** Colinearity: how the target variable is correlated with the other features (variables)')\n",
    "    print('** Multi-colinearity: how each feature correlates among themselves (multi-colinearity)')\n",
    "\n",
    "    print('\\n')\n",
    "    print('*** Heatmap: Pearson Correlation ***')\n",
    "    print(f'It evaluates the linear relationship between two continuous variables \\n'\n",
    "          f'* A +ve correlation indicates that as one variable increases the other variable tends to increase.\\n'\n",
    "          f'A correlation near zero indicates that as one variable increases, there is no tendency in the other variable to either increase or decrease.\\n'\n",
    "          f'A -ve correlation indicates that as one variable increases the other variable tends to decrease.')\n",
    "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print('\\n')\n",
    "    print(f'*** Heatmap: Spearman Correlation ***')\n",
    "    print(f'It evaluates monotonic relationship \\n'\n",
    "          f'Spearman correlation coefficients range from -1 to +1.\\n'\n",
    "          f'The sign of the coefficient indicates whether it is a positive or negative monotonic relationship.\\n'\n",
    "          f'* A positive correlation means that as one variable increases, the other variable also tends to increase.')\n",
    "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print('\\n')\n",
    "    print('*** Heatmap: Power Predictive Score (PPS) ***')\n",
    "    print(f'PPS detects linear or non-linear relationships between two columns.\\n'\n",
    "          f'The variable on the x-axis is used to predict the corresponding variable on the y-axis.\\n'\n",
    "          f'The score ranges from 0 (no predictive power) to 1 (perfect predictive power)\\n\\n'\n",
    "          f'* PPS == 0 means that there is no predictive power\\n'\n",
    "          f'* PPS < 0.2 often means that there is some relevant predictive power but it is weak\\n'\n",
    "          f'* PPS > 0.2 often means that there is strong predictive power\\n'\n",
    "          f'* PPS > 0.8 often means that there is a deterministic relationship in the data\\n')\n",
    "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = df_train_transformed.drop(['Data_No', 'Flow_rate', 'Dust', 'mass_g', 'Dust_feed', 'Tt', 'bin_size', 'median_DP', 'std_DP', 'change_DP', 'change_EWM'], axis=\"columns\")\n",
    "df_corr_pearson, df_corr_spearman, pps_matrix = calculate_corr_and_pps(df_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_corr_and_pps(df_corr_pearson = df_corr_pearson, df_corr_spearman = df_corr_spearman,\n",
    "                    pps_matrix = pps_matrix, CorrThreshold = 0, PPS_Threshold =0,\n",
    "                    figsize=(12,10), font_annot=10\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Maintaining the observations we have previously made at 02_DataCleaning notebook, we also note:\n",
    "\n",
    "* As we would think, there is a strong linear relationship between **Exponential Weighted Mean** (4pointEWM) and **Differential Pressure** (DP), indicating that we can confidently consider 4pointEWM (and its log value) as a proxy for DP.\n",
    "    * Subsequently there is a strong **inverse** linear relationship between Exponential Weighted Mean (4pointEWM) and **Remaining Filter Balance**.\n",
    "    \n",
    "* Cumulative mass and time have a mildly positive relationship to Differential Pressure values, however their power to predict it is considered **none** to very weak at best."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SmartCorrelatedSelection** of Variables\n",
    "\n",
    "[Feature Engine](https://pypi.org/project/feature-engine/) is a python library with multiple transformers to engineer and select features for use in machine learning models. \n",
    "\n",
    "`pip install feature-engine`\n",
    "\n",
    "Applied here to confirm our conclusions, it looks for groups of features that correlate amongst themselves and removes any surplus correlated features since they add the same information to the model.\n",
    "\n",
    "The transformer finds the groups and drops the features based on the **method**, **threshold** and **selection** method we choose.\n",
    "\n",
    "For every group of correlated features, the transformer will remove all but one feature.\n",
    "\n",
    "* Step 1: Create a separate DataFrame, with the variable(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_engineering = df_total.copy().drop(['change_DP', 'change_EWM'], axis=\"columns\")\n",
    "df_engineering.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 2: Create engineered variables(s) applying the transformation(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")\n",
    "\n",
    "corr_sel.fit_transform(df_engineering)\n",
    "# corr_sel.correlated_feature_sets_\n",
    "print('Correlated Variables :\\n', corr_sel.correlated_feature_sets_)\n",
    "print('\\nFeatures to Drop :\\n', corr_sel.features_to_drop_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On running the transformer, it found 2 groups of correlated features:\n",
    "1. `'DeviceProtection', 'OnlineBackup', 'OnlineSecurity', 'TechSupport',`\n",
    "2. `'mass_g', 'Tt', 'Dust_feed', 'RUL'`\n",
    "\n",
    "* It decided `Differential_pressure` was the most relevant feature to keep in **group 1**.\n",
    "* It decided `Dust_feed` was the most relevant feature to keep in **group 2**.\n",
    "\n",
    "### Observations\n",
    "* Confirmed **Differential Pressure** as a primary feature in both **train** & **test** datasets\n",
    "* Confirmed **Dust_feed** as a primary feature in the **test** dataset.\n",
    "* We can include these steps to each ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Datasets \n",
    "Save the files to /transformed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "  os.makedirs(name='outputs/datasets/transformed')\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "df_train.to_csv(f'outputs/datasets/transformed/dfPreTransformTrain.csv',index=False)\n",
    "df_train_transformed.to_csv(f'outputs/datasets/transformed/dfTransformedTrain.csv',index=False)\n",
    "df_test.to_csv(f'outputs/datasets/transformed/dfTransformedTest.csv',index=False)\n",
    "df_total.to_csv(f'outputs/datasets/transformed/dfTransformedTotal.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Iteration\n",
    "In this project, feature iteration has been included as part of each modelling process as there is no one model family that works best for every problem. Feature iteration will however follow a process of repeating a set of tasks to achieve a result.\n",
    "\n",
    "#### **Fitting Parameters** \n",
    "Each model has its own parameters that define it which may change depending on the type of model it is; \n",
    "* A regression model may be defined by its feature coefficients\n",
    "* A decision tree may be defined by its branch locations\n",
    "* A neural network may be defined by the weights connecting its layers\n",
    "\n",
    "We can determine the appropriate values for the parameters for each model using iterative methods to find the minimum of a function. This is commonly known the gradient descent or loss (or cost) function and will be implemented using the [Scikit-Learn](https://scikit-learn.org/stable/about.html#citing-scikit-learn) library.\n",
    "\n",
    "#### **Tuning Hyperparameters**\n",
    "* Each model sits in a category of models (family) with customizable structures\n",
    "* Each set of structural choices (hyperparameters) must be chosen before fitting the models parameters.\n",
    "\n",
    "We can build separate models with different structural choices. We use these to investigate the different results and evaluate which is best for what it is we are trying to achieve. This can be done with **Cross-Validation**, an iterative method for evaluating the performance of models built with a given set of hyperparameters.\n",
    "\n",
    "When we **train** a model, we:\n",
    "1. Decide hyperparameters for the model family: e.g. Should the model have a penalty to prevent overfitting?\n",
    "2. Fit the model parameters to the data: e.g. What are the model coefficients that minimize the loss function?\n",
    " \n",
    "#### **Attending to the Problem**\n",
    "Techniques to attend to those easy to solve issues (low-hanging-fruit) that improve predictive performance. Things like:\n",
    "* Trying Different Model Families\n",
    "* Combining multiple models into an ensemble\n",
    "\n",
    "#### **Improving Our Data**\n",
    "**Collecting Better Data**:\n",
    "* Not included as part of this project, as the data has been artificially sourced, however in the workplace we may look at iterating through the data and or data collection process to improve the depth, quality or quantities of data and even collect alternate variables, like filter brand or all tests to the point of filter failure.\n",
    "\n",
    "**Engineering Better Features**\n",
    "* By leveraging on the knowledge of subject matter experts, creating new features from the data to improve the model performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Next steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions: \n",
    "* Data has been cleaned and engineered with a variety of methods\n",
    "* Changes in Differential Pressure, its Exponential Weighted Mean (4pointEWM), Time and dust type seem to be the leading parameters to consider for us to create a predictive model. \n",
    "\n",
    "#### Next Steps:\n",
    "* Regression Models\n",
    "    * With Classification Models as required\n",
    "* Cluster Model\n",
    "* Correlation Study"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
