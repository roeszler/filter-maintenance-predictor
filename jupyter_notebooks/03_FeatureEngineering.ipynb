{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI Portfolio Project 5 - Filter Maintenance Predictor 2022\n",
    "## **Feature Engineering Notebook**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Cleaning**\n",
    "\n",
    "Performed within the [data cleaning notebook](https://github.com/roeszler/filter-maintenance-predictor/blob/main/jupyter_notebooks/02_DataCleaning.ipynb).\n",
    "\n",
    "**2. Data Transformation**\n",
    "* Processing the data for the modelling stage.\n",
    "* Transform data into a format that is useful for the algorithm learn the relationship among the variables.\n",
    "* Evaluate the use of the following approaches to engineer the variables:\n",
    "    * ordinal categorical encoding\n",
    "    * numerical transformation\n",
    "    * smart correlated selection\n",
    "    \n",
    "**3. Feature Extraction**\n",
    "* Evenly distribute dust type\n",
    "\n",
    "**4. Feature Selection**\n",
    "\n",
    "**5. Feature Iteration**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inputs\n",
    "\n",
    "1. Cleaned Test Dataset : `outputs/datasets/collection/dfCleanTrain.csv`\n",
    "\n",
    "2. Cleaned Train Dataset : `outputs/datasets/collection/dfCleanTrain.csv`\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* Generate engineered Train and Test sets, both saved under `outputs/datasets/transformed`\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "  * Best approach to engineer variables based on...\n",
    "  * Transformations that we will consider in our pipeline are..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"Current directory set to new location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_total = pd.read_csv(f'outputs/datasets/cleaned/dfCleanTotal.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "## Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to define\n",
    "* Feature Extraction: **creates** new features from functions of the original features.\n",
    "* Feature Selection: allows us to **choose** a subset of the features for use in model construction.\n",
    "\n",
    "Semantically we could argue each are a **new feature** or a **subset** of the feature. We have however considered any value intended for use in model construction as a **Selected Feature**: *The process of selecting a subset of relevant features (variables, predictors) for use in model construction*. \n",
    "\n",
    "Feature selection techniques are used for several reasons:\n",
    "\n",
    "* Simplification of models to make them easier to interpret by researchers/users\n",
    "* Shorter training times\n",
    "* Avoiding too many input variables (dimensionality)\n",
    "* Improve the data's compatibility with a learning model class\n",
    "* Create symmetries in the input data.\n",
    "\n",
    "The main idea when using a feature selection is that the data contains some features that are either **surplus**, **redundant** or **irrelevant** to the final business goal and can therefore be removed without incurring much loss of predictive power."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Categorical encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert `Data_No` to a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_no_total = df_total['Data_No'].map(str)\n",
    "# df_total['Data_No'] = data_no_total\n",
    "# df_total.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical transformations\n",
    "\n",
    "This process can consider transformers like:\n",
    "* Smooth with SMA\n",
    "* Logarithmic in base e\n",
    "* Logarithmic in base 10\n",
    "* Reciprocal\n",
    "* Power\n",
    "* BoxCox\n",
    "* Yeo Johnson\n",
    "\n",
    "Note: Part of the challenge with this data is dealing with a continuous dataset that is comprised of data 'bins'. Each `Data_No` data bin represents an individual test cycle. \n",
    "\n",
    "When engineering **descriptive statistics** into the dataset, that typically used a sample of the previous values to indicate a mean or standard deviation, **we need to treat each bin individually** and not progress the first row of each bin incorrectly with the last rows of the previous bin.\n",
    "\n",
    "To solve this, we use a sequential loop that proceeds through each bin and inserts a progressive descriptive statistic calculation like change in differential pressure (`change_DP`) and appends it to the previous bin. This eventually creates a version of the dataset that includes the new calculation:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change in Differential Pressure\n",
    "Include change in Differential Pressure calculation\n",
    "\n",
    "**Note**: We replace first instance of `change_DP` with first value of `differential_pressure` in the new bin. \n",
    "* This signifies that the fist observation starts from a **zero** DP value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_change_dp = pd.DataFrame()\n",
    "\n",
    "list_data_nos = list(df_total['Data_No'].unique())\n",
    "for n in list_data_nos:\n",
    "    if (df_total.Data_No != df_total.Data_No.shift(1)).any().any():\n",
    "        df_bin = df_total[df_total['Data_No'] == n]\n",
    "\n",
    "        change_dp_calc = df_bin['Differential_pressure'].diff().fillna(df_bin['Differential_pressure'])\n",
    "        df_bin.insert(loc=7, column='change_DP', value=change_dp_calc)\n",
    "\n",
    "        df_change_dp = pd.concat([df_change_dp, df_bin], ignore_index = True)\n",
    "df_total = df_change_dp\n",
    "df_total.loc[446:451]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers in differential pressure observations\n",
    "\n",
    "In each bin we notice that the size and direction of the `change_DP` measure occasionally produces a zero or negative value. This highlights a fluctuation in the **differential pressures**. These may be considered outliers as the pressure gradient across the filter needs time to stabilize. We have considered three main methods to deal with these observations:\n",
    "* Log transformation\n",
    "* Winsorize method\n",
    "* Dropping the outliers\n",
    "\n",
    "These will be handled in the [feature engineering](https://github.com/roeszler/filter-maintenance-predictor/blob/main/jupyter_notebooks/03_FeatureEngineering.ipynb) notebook\n",
    "\n",
    "**Random sample** (Bin 96) to plot and inspect change in Differential Pressure distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,5))\n",
    "\n",
    "bin_96 = df_total[df_total['Data_No'] == 96]\n",
    "sns.boxplot(x = bin_96['change_DP'], ax=ax1)\n",
    "sns.histplot(x = bin_96['change_DP'], ax=ax2)\n",
    "sns.lineplot(x=bin_96['Time'], y=bin_96['Differential_pressure'], ax=ax3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing of **Differential Pressure**\n",
    "*Dealing with outliers in differential pressure observations*\n",
    "\n",
    "In each bin note `differential_pressure` and `change_DP` observations. Occasionally the measures fluctuate outside of the general trend of the data (outliers). This can be seen below in the **second to last** observation of Data_No bin 98 (index no. 75826) where the recorded value is outside the general trend of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total['Data_No'] == 98].tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- To quantify such measures, have considered a tolerance of ±200% change in value, however this can be altered depending on what we see when fitting the models. -->\n",
    "\n",
    "To **smooth** the variability of this measure we can apply a **mean** (or average) value to the data in various ways. \n",
    "This attempts to soften the severity of changes seen and reduce the instances of values that are higher / lower than the general trend. It will effectively reduce variability in the **differential pressure** measure, making it easier to model.\n",
    "\n",
    "We have considered the following methods to deal with outliers:\n",
    "* Smooth with Simple Mean Average (SMA)\n",
    "* Exponentially Weighted Mean (EWM)\n",
    "* Log transformation\n",
    "* Dropping the outliers\n",
    "* Winsorize method\n",
    "\n",
    "For each calculation that uses previous observations to produce a value, we can **use the same process we applied to manage the unique data bins** as we did in calculating change in differential pressure above.\n",
    "\n",
    "#### Smoothing of Differential Pressure with a **Simple Moving Average** (SMA)\n",
    "A simple moving average (SMA) is an arithmetic **moving average** calculated by adding recent values (the last four measures in this case), then dividing that by the number of observations in the calculation. This moves along as the observations progresses.\n",
    "\n",
    "* We can see the first four observations are **NaN** indicated. \n",
    "    * These could be imputed with arbitrary values, mean values, closest k sample values and/or a MICE (Multiple Imputation by Chained Equations) value that fits a linear regression with the present values.\n",
    "    * On review, the MICE method would be the preferred method to impute the missing SMA values, **however**: considering the progressive nature of the `differential_pressure` variable, a preferable alternate to SMA would be Exponentially Weighted Mean (EWM).\n",
    "\n",
    "#### Smoothing of Differential Pressure with an **Exponentially Weighted Mean** (EWM)\n",
    "Is a measure of the moving average that considers older observations to have given lower weightings in the calculation. The weights fall exponentially as the data point gets older – hence the name exponentially weighted.\n",
    "\n",
    "* The EWM is calculated with a 4 point EWM (`span=4`) measure, that considers the previous 4 observations to calculate the weighted mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means = pd.DataFrame()\n",
    "\n",
    "list_data_nos = list(df_total['Data_No'].unique())\n",
    "for n in list_data_nos:\n",
    "    if (df_total.Data_No != df_total.Data_No.shift(1)).any().any():\n",
    "        df_bin = df_total[df_total['Data_No'] == n]\n",
    "\n",
    "        ewm_calc = df_bin['Differential_pressure'].ewm(span=4, adjust=False).mean()\n",
    "        df_bin.insert(loc=2, column='4point_EWM', value=ewm_calc)\n",
    "\n",
    "        sma_calc = df_bin['Differential_pressure'].rolling(4).mean()\n",
    "        df_bin.insert(loc=2, column='4point_SMA', value=sma_calc)\n",
    "\n",
    "        change_ewm_calc = df_bin['4point_EWM'].diff().fillna(df_bin['4point_EWM'])\n",
    "        df_bin.insert(loc=10, column='change_EWM', value=change_ewm_calc)\n",
    "\n",
    "        df_means = pd.concat([df_means, df_bin], ignore_index = True)\n",
    "df_total = df_means\n",
    "df_total.loc[446:451]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logarithmic Transformation** of Differential Pressure\n",
    "\n",
    "As seen at data collection, the original continuous `differential_pressure` data is right or **positively skewed** at 1.81 and does not follow the shape of a normally distributed bell curve.\n",
    "* Included a plot for both instances of the **moving average smoothed** variable to check that it is representative of the **differential_pressure** source, which it is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,6))\n",
    "\n",
    "sns.histplot(x = df_total['Differential_pressure'], ax=ax1)\n",
    "sns.histplot(x = df_total['4point_SMA'], ax=ax2)\n",
    "sns.histplot(x = df_total['4point_EWM'], ax=ax3)\n",
    "\n",
    "ax1.title.set_text('Differential Pressure Histogram\\n')\n",
    "ax2.title.set_text('Simple Moving Average Histogram\\n')\n",
    "ax3.title.set_text('Exponential Weighted Mean Histogram\\n')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **log transformation** of this data will represent the values within a normal distribution, **as much as possible**, allowing a more valid statistical analysis from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,6))\n",
    "\n",
    "df_total_log_dp = df_total['Differential_pressure']\n",
    "log_dp = np.log(df_total_log_dp)\n",
    "sns.histplot(x = log_dp, ax=ax1)\n",
    "\n",
    "df_total_log_sma = df_total['4point_SMA']\n",
    "log_sma = np.log(df_total_log_sma)\n",
    "sns.histplot(x = log_sma, ax=ax2)\n",
    "\n",
    "df_total_log_ewm = df_total['4point_EWM']\n",
    "log_ewm = np.log(df_total_log_ewm)\n",
    "sns.histplot(x = log_ewm, ax=ax3)\n",
    "\n",
    "# plt.title('Log Histogram')\n",
    "ax1.title.set_text('Log_DP Histogram\\n')\n",
    "ax2.title.set_text('Log_SMA Histogram\\n')\n",
    "ax3.title.set_text('Log_EWM Histogram\\n')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** \n",
    "* The shape of the numerical differential pressure data is **improved** by a natural logarithmic transformation.\n",
    "    <!-- * Untreated values of 85.71 mean, std.deviation at 114.34 and median at 38.34, indicates the positively skewed nature of the original data\n",
    "    * Transformed the data is much improved with mean 3.18 much closer to the median 3.89 -->\n",
    "\n",
    "The transformed data is still affected by the negative skew in the data:\n",
    "* Pressure measures show values at zero or below at the beginning and tail of the data. This is amplified when we transform the mean data sets.\n",
    "    * The instances of negative numbers can be managed by:\n",
    "        * Adding a constant to stop the value becoming negative, or \n",
    "        * Indicate the negative as a missing number or \n",
    "        * Dropping the entire observation\n",
    "    * Considering these measures tend to fall in zones at the beginning or end of the test where the procedure takes a while to 'equalize' or has passed the point of filter failure, we have decided to treat them as outliers and confidently drop the entire row these observation sits within without impacting the power of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "# df.insert(loc=0, column='log_EWM', value=(log_ewm*10))\n",
    "# df.insert(loc=0, column='log_SMA', value=(log_sma*10))\n",
    "# df.insert(loc=0, column='log_DP', value=(log_dp*10))\n",
    "# df.insert(loc=0, column='DP', value=df_total_log_dp)\n",
    "# round(df.describe(), 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further EWM transformation**\n",
    "\n",
    "Remove rows with negative numbers in `4pointWEM` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_total.insert(loc=4, column='log_EWM', value=log_ewm)\n",
    "# data = df_total.loc[:, df_total.columns == 'log_EWM']\n",
    "# df_total = df_total[data.select_dtypes(include=[np.number]).ge(-0).all(1)]\n",
    "# # df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Negative values\n",
    "df_total.insert(loc=4, column='log_EWM', value=log_ewm)\n",
    "data = df_total.loc[:, df_total.columns == 'log_EWM']\n",
    "df_total = df_total[data.select_dtypes(include=[np.number]).ge(-0).all(1)]\n",
    "\n",
    "# Visualize the data\n",
    "old_shape = pd.read_csv(f'outputs/datasets/cleaned/dfCleanTotal.csv')\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "sns.histplot(x = log_dp, ax=ax1)\n",
    "sns.histplot(x = df_total['log_EWM'], ax=ax2)\n",
    "\n",
    "plt.title('log_EWM Histogram')\n",
    "ax1.title.set_text('Transformed Differential Pressure Histogram\\n')\n",
    "ax2.title.set_text('Transformed Differential Pressure Exponentially Weighted Mean Histogram (Values > 0)\\n')\n",
    "plt.show()\n",
    "print(\"Old Shape: \", old_shape.shape)\n",
    "print(\"New Shape: \", df_total.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformed data is far more useable to train the model. \n",
    "* Less variability \n",
    "* Negative values removed\n",
    "* Minimal loss of data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Management of Outliers\n",
    "Dropping all negative numbers to reduce the noise of the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of detecting outliers using **Inter Quartile Range** (IQR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_IQR = df_total.copy()\n",
    "\n",
    "# IQR\n",
    "Q1 = np.percentile(df_total_IQR, 25, method='midpoint')\n",
    "Q3 = np.percentile(df_total_IQR, 75,method='midpoint')\n",
    "IQR = Q3 - Q1\n",
    "print(\"Old Shape: \", df_total_IQR.shape)\n",
    " \n",
    "# Upper bound\n",
    "upper = np.where(df_total_IQR >= (Q3+1.5*IQR))\n",
    "# Lower bound\n",
    "lower = np.where(df_total_IQR <= (Q1-1.5*IQR))\n",
    " \n",
    "''' Removing the Outliers '''\n",
    "df_total_IQR.drop(upper[0], inplace = True)\n",
    "df_total_IQR.drop(lower[0], inplace = True)\n",
    " \n",
    "print(\"New Shape: \", df_total_IQR.shape)\n",
    "\n",
    "# df_total_log_IQR = df_total_IQR['Differential_pressure']\n",
    "# df_total_log_IQR = df_total_IQR['4point_SMA']\n",
    "# df_total_log_IQR = df_total_IQR['4point_EWM']\n",
    "df_total_log_IQR = df_total_IQR['log_EWM']\n",
    "log_dp_IQR = np.log(df_total_log_IQR)\n",
    "# df_total_IQR['log_DP'] = log_dp\n",
    "sns.histplot(x = df_total_log_IQR)\n",
    "plt.title('IQR Histogram of log EWM')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR transformation is not particularly effective on this data with negative values removed. We will not apply this method and proceed with other techniques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The Winsorize Method**\n",
    "Winsorization is the process of replacing the extreme values of statistical data in order to limit the effect of the outliers on the calculations or the results obtained by using that data. To apply this to a our exponentially logged differential pressure `log_EWM` variable, where outliers are present only at one end of the data:\n",
    "* The lower 10% values of the data will have their values set equal to the value of the data point at the 10th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "WinsorizedArrayMean = np.mean(df_total['log_EWM'])\n",
    "winz_EWM = winsorize(df_total['log_EWM'],(0.1,0.1))\n",
    "df_total.insert(loc=5, column='winz_EWM', value=winz_EWM)\n",
    "WinsorizedArray = df_total['winz_EWM']\n",
    "plt.boxplot(WinsorizedArray)\n",
    "plt.title('Winsorized array')\n",
    "plt.show()\n",
    "WinsorizedArrayNewMean = np.mean(WinsorizedArray)\n",
    "print('Old Mean: ', WinsorizedArrayMean)\n",
    "print('New Mean: ', WinsorizedArrayNewMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_log_winz = df_total['winz_EWM']\n",
    "log_winz = np.log(df_total_log_winz)\n",
    "sns.histplot(x = log_winz)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winzorization smooths the data too much and does not add much value to training our models so we will not apply this method either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_total['winz_EWM']\n",
    "del df_total['4point_SMA']\n",
    "# del df_train['winz_Mean']\n",
    "# del df_train['log_DP']\n",
    "df_total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evenly distribute dataset by `Dust` type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the **train** and **test** sets supplied have data distributed unevenly between 50 test bins. To account for this we wish to assess the measures of central tendency for each Dust class, with the aim of reducing the data size to a more evenly proportioned one between classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train** Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considerations**\n",
    "\n",
    "* The proportion of data that **has reached filter failure** is represented by how close `filter_balance` is to zero or less. \n",
    "    * Data with filter_balance values approaching zero may be worth keeping and will make part of our heuristic decision process.\n",
    "* Notwithstanding that the **mean** is the most frequently used measure of central tendency because it uses all values in the data set to give you an average\n",
    "    * For data from skewed distributions (like `differential_pressure`), the **median** is better than the mean because it isn’t influenced by extremely large values.\n",
    "\n",
    "In the following calculation, we see a summary of the top ten `Data_No` bins where `differential_pressure` observations that have made it to the **600 Pa** (the point of filter failure)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide back into **Train** and **Test** sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_total = df_total['Data_No'].map(int).round(decimals=0)\n",
    "df_total.loc[:, 'Data_No'] = data_no_total\n",
    "\n",
    "n = df_total['Data_No'].iloc[0:len(df_total)]\n",
    "df_train = df_total[n < 51].reset_index(drop=True, names='index')\n",
    "df_test = df_total[n > 50].reset_index(drop=True, names='index')\n",
    "del df_train['RUL']\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_row_train = df_train[df_train.Data_No != df_train.Data_No.shift(-1)]\n",
    "# last_row_descending = last_row_train.sort_values(by='Dust', ascending=True)\n",
    "last_row_descending = last_row_train.sort_values(by='Differential_pressure', ascending=False)\n",
    "last_row_descending.head(n=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the diagram below showing proportions of `Dust` variable in the **df_train** dataset.\n",
    "* It shows a disproportionate mix between classes. This will be the first dataset we tidy up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "category_totals = df_train.groupby('Dust')['Differential_pressure'].count().sort_values()\n",
    "category_totals.plot(kind=\"barh\", title='Proportion of Dust Classes in df_train\\n', xlabel='\\nObservations', ylabel='Dust Class')\n",
    "category_totals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our next aim is to \n",
    "* Fill these bins with data that best represents a central tendency.\n",
    "* Make the size of each bin around ±**6100** observations (similar to the A4 Coarse Dust class bin) \n",
    "\n",
    "#### Procedure\n",
    "* Include a comparison to how far each `differential_pressure` measure **deviates** or how far it is from the **.median()** value of the bin.\n",
    "* Ordered by `filter_balance` showing sets with data closest to 600 Pa `differential_pressure`.\n",
    "* Include comparison to median\n",
    "* Add a cumulative measure of Data_No's to use as a ranking\n",
    "* Create a dataframe of the A3 Medium Dust : **1.025**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a calculation of Standard Deviation to **df_train** test set and hide several colums for easier viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df_train['winz_Mean']\n",
    "# del df_train['log_DP']\n",
    "std_group = df_train.groupby('Data_No').std()\n",
    "std_group.index.name = None\n",
    "std_group.loc[:,'Data_No'] = std_group.index\n",
    "map_std = df_train['Data_No'].map(std_group.set_index('Data_No')['Differential_pressure'])\n",
    "df_train.loc[:,'std_DP'] = map_std\n",
    "# df_test.loc[363:368]\n",
    "df_train.loc[30082:30087].style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a calculation of Coefficient of Variation (variance) to **df_train** test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cv = lambda data: np.std(data, ddof=1) / np.mean(data, axis=0) * 100 \n",
    "var_group = df_train.groupby('Data_No').apply(cv)\n",
    "var_group.index.name = None\n",
    "var_group.loc[:,'Data_No'] = var_group.index\n",
    "map_var = df_train['Data_No'].map(var_group.set_index('Data_No')['Differential_pressure'])\n",
    "df_train.loc[:,'cv_DP'] = map_var\n",
    "# df_test.loc[363:368]\n",
    "df_train.loc[444:453]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of variation is an indication of how far the standard deviation is away from the mean. As we can see it does not add value to our understanding of the data, primarily due the the skewed nature of the `differential_pressure` continuous variable. \n",
    "* This re-enforces the understanding that descriptive statistics using the mean may not be preferred measure of central tendency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Coefficient of Variation and Add Median to df_train**\n",
    "* Median is the preferred measure of central tendency to observe in a skewed dataset such as this as it is not as affected by larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train['cv_DP']\n",
    "median_group = df_train.groupby('Data_No').median()\n",
    "median_group.index.name = None\n",
    "median_group.loc[:,'Data_No'] = median_group.index\n",
    "map_median = df_train['Data_No'].map(median_group.set_index('Data_No')['Differential_pressure'])\n",
    "df_train.loc[:,'median_DP'] = map_median\n",
    "# df_train.loc[444:453]\n",
    "df_train.loc[444:453].head().style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can evaluate the dataframe with just **A3 Dust** in it, ordered by `filter_balance` as a measure of time to filter failure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the size of each bin and include a cumulative sum of each **bin size** to help see which data bin that reaches **6100** or more total values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_sum = df_train.groupby('Data_No')['Data_No'].count().reset_index(name='bin_tot')\n",
    "map_bin = df_train['Data_No'].map(bin_sum.set_index('Data_No')['bin_tot'])\n",
    "df_train.loc[:, 'bin_size'] = map_bin\n",
    "\n",
    "dust_A3 = df_train[df_train['Dust'] == 1.025]\n",
    "filter_A3 = dust_A3[dust_A3.Data_No != dust_A3.Data_No.shift(-1)]\n",
    "df_train_A3 = filter_A3.sort_values(by='filter_balance', ascending=True)\n",
    "\n",
    "df_train_A3['c_sum'] = df_train_A3['bin_size'].cumsum()\n",
    "dn_fb = df_train_A3.loc[:, 'Data_No'].head(14).sort_values(ascending=True).reset_index(drop=True)\n",
    "df_train_A3.head(14).style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the current dataframe containing only A3 Medium Dust observations, that is ordered by those tests with closest to a completed test to failure:\n",
    "* **The top 9 data bins (seen at bin 7) would extract a A3 Medium dust training dataset with 7,208 observations**\n",
    "* We will now perform a further PDA to evaluate the suitability of these further"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank by Standard Deviations, ordered by `std_DP`\n",
    "The standard deviation is used to measure the spread of values in a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dust_A3 = df_train[df_train['Dust'] == 1.025]\n",
    "# filter_A3 = dust_A3[dust_A3.Data_No != dust_A3.Data_No.shift(-1)]\n",
    "df_train_A3_std = filter_A3.sort_values(by='std_DP', ascending=True)\n",
    "df_train_A3_std['c_sum'] = df_train_A3_std['bin_size'].cumsum()\n",
    "dn_sdv = df_train_A3_std.loc[:, 'Data_No'].head(14).sort_values(ascending=True).reset_index(drop=True)\n",
    "df_train_A3_std.head(14).style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank by central tenancy of the Median value, ordered by `median_DP`\n",
    "* Median = the value of the number in the middle of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dust_A3 = df_train[df_train['Dust'] == 1.025]\n",
    "# filter_A3 = dust_A3[dust_A3.Data_No != dust_A3.Data_No.shift(-1)]\n",
    "df_train_A3_median = filter_A3.sort_values(by='median_DP', ascending=False)\n",
    "df_train_A3_median['c_sum'] = df_train_A3_median['bin_size'].cumsum()\n",
    "dn_mdp = df_train_A3_median.loc[:, 'Data_No'].head(14).sort_values(ascending=True).reset_index(drop=True)\n",
    "df_train_A3_median.head(14).style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review of the **data bin numbers** across descriptors of central tendency that would capture **6100** or more total values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_review = pd.DataFrame()\n",
    "dn_review.insert(loc=0, column='by_std_DP', value=dn_sdv)\n",
    "dn_review.insert(loc=0, column='by_median_DP', value=dn_mdp)\n",
    "dn_review.insert(loc=0, column='by_filter_balance', value=dn_fb)\n",
    "dn_review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations\n",
    "\n",
    "* We can see that when we filter by **filter balance** or **median differential pressure** that most bin numbers are contained in both. In fact only bins 1, 16, and 24 are not contained in both, se we can consider these as the first to re-introduce to the data, perhaps when creating the validation set. \n",
    "    * This provides us with confidence that these test are higher value for our purposes and promote inclusion into our A3 Dust group.\n",
    "* Reviewing those values with the lowest **Standard Deviation** inadvertently includes selects values furthest from fully completing a test (ie differential pressure reaching > 600 Pa). In such, these bins will not factor highly in our considerations to include in the dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "* A **Selected Feature**: *The process of selecting a subset of relevant features (variables, predictors) for use in model construction*. \n",
    "\n",
    "Feature selection techniques are used for several reasons:\n",
    "\n",
    "* Simplification of models to make them easier to interpret by researchers/users\n",
    "* Shorter training times\n",
    "* Avoiding too many input variables (dimensionality)\n",
    "* Improve the data's compatibility with a learning model class\n",
    "* Create symmetries in the input data.\n",
    "\n",
    "The main idea when using a feature selection is that the data contains some features that are either **surplus**, **redundant** or **irrelevant** to the final business goal and can therefore be removed without incurring much loss of predictive power."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting bins from the **Training** dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a separate frame indicating the bin numbers we wish to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_no = df_train_A3['Data_No'].head(9)\n",
    "bin_no.to_frame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these references to create a dataframe `df_train_cleaned_A3` that is ready for inclusion in our final dataframe `df_train_clean`.\n",
    "* Note we disregard the cumulative sum measure as it doesn't add value to further calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train\n",
    "df_train_cleaned_A3 = df_train_copy[df_train_copy['Data_No'].isin(bin_no)]\n",
    "df_train_cleaned_A3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Quick Review: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape started with: ', dust_A3.shape)\n",
    "print('Shape we have now: ', df_train_cleaned_A3.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add these to a new dataset to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_A2 = df_train[df_train['Dust'] == 0.900]\n",
    "dust_A3 = df_train_cleaned_A3\n",
    "dust_A4 = df_train[df_train['Dust'] == 1.200]\n",
    "\n",
    "df_train_compare = pd.concat([dust_A2, dust_A3, dust_A4], ignore_index = True)\n",
    "df_train_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "category_totals = df_train_compare.groupby('Dust')['Differential_pressure'].count().sort_values()\n",
    "category_totals.plot(kind=\"barh\", title='Proportion of Dust Classes in \"df_train_compare\"\\n', xlabel='\\nObservations', ylabel='Dust Class')\n",
    "category_totals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat procedure with remaining `Dust` classes and **Test** dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract, Clear and Replace..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_sum = df_train.groupby('Data_No')['Data_No'].count().reset_index(name='bin_Tot')\n",
    "# map_bin = df_train['Data_No'].map(bin_sum.set_index('Data_No')['bin_Tot'])\n",
    "# df_train.loc[:, 'bin_Size'] = map_bin\n",
    "\n",
    "# df_train.loc[38817:38827]\n",
    "\n",
    "dust_A2 = df_train[df_train['Dust'] == 0.900]\n",
    "filter_A2 = dust_A2[dust_A2.Data_No != dust_A2.Data_No.shift(-1)]\n",
    "df_train_A2 = filter_A2.sort_values(by='filter_balance', ascending=True)\n",
    "df_train_A2['c_sum'] = df_train_A2['bin_size'].cumsum()\n",
    "df_train_A2.head(13).style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_no = df_train_A2['Data_No'].head(9)\n",
    "bin_no.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train\n",
    "df_train_cleaned_A2 = df_train_copy[df_train_copy['Data_No'].isin(bin_no)]\n",
    "df_train_cleaned_A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_A2 = df_train_cleaned_A2\n",
    "dust_A3 = df_train_cleaned_A3\n",
    "dust_A4 = df_train[df_train['Dust'] == 1.200]\n",
    "\n",
    "df_train_compare = pd.concat([dust_A2, dust_A3, dust_A4], ignore_index = True)\n",
    "df_train_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "category_totals = df_train_compare.groupby('Dust')['Differential_pressure'].count().sort_values()\n",
    "category_totals.plot(kind=\"barh\", title='Proportion of Dust Classes in \"df_train_compare\"\\n', xlabel='\\nObservations', ylabel='Dust Class')\n",
    "category_totals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How much data do we need**?\n",
    "* At a bin sample of 9 for both A2 and A3 dust, we currently have a little under 15,000 observations to train the ML Models\n",
    "* Will review this depending on the performance of the models\n",
    "* Overfitting with too much data vs underfitting with too little is a balance and the qualitative nature of the heuristics surrounding the predictions made\n",
    "* Nonlinear Algorithms (like clustering) may need more data\n",
    "\n",
    "Short answer is, **we have plenty of extra data at this point that may be less suited to training the model**. \n",
    "\n",
    "* Notwithstanding, it is still live data and may there may be some value toward increasing the power of our ML Models. \n",
    "* We also have the option to alter the selected number of dust class bins and possibly augmenting the smaller A4 dust dataset with the **Synthetic Minority Oversampling Technique (SMOTE)** or SMOTE NC for categorical data should we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_cleaned = df_train_compare\n",
    "# df_train_cleaned\n",
    "df_train = df_train_compare\n",
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could run the **test data** through the above process, however it is intended to *test* the models and make sure they’ll work when new data is introduced. We do not want to taylor this data too much as it may promote overfitting the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Feature Selection Considerations\n",
    "\n",
    "Popular filter metrics for **classification problems** are \n",
    "* **Correlation-based feature selection**\n",
    "* Mutual information\n",
    "* Class separability\n",
    "    * Error probability\n",
    "    * Inter-class distance\n",
    "    * Probabilistic distance\n",
    "    * Entropy\n",
    "* **Consistency-based feature selection**\n",
    "* **Correlation-based feature selection**\n",
    "\n",
    "#### Correlation Based Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot relevant relationships to **train** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_train_corr = df_train.drop(['Data_No', 'Flow_rate', 'Dust', 'mass_g', 'Dust_feed', 'Tt', 'bin_size', 'median_DP', 'std_DP', 'change_DP', 'change_EWM'], axis=\"columns\")\n",
    "sns.pairplot(data=df_train_corr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot relevant relationships to **test** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_corr = df_test.drop(['Data_No', 'Flow_rate', 'Dust', 'mass_g', 'Dust_feed', 'Tt', 'change_DP', 'change_EWM'], axis=\"columns\")\n",
    "sns.pairplot(data=df_test_corr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a quick look, we notice the same relationships between `differential_pressure`, `RUL` and `Time`, with our engineered variables also indicating influence.\n",
    "\n",
    "#### Heatmaps for **df_train** engineered dataset\n",
    "\n",
    "Import ppscore function (Code Institute [Exploratory Data Analysis Tools](https://learn.codeinstitute.net/courses/course-v1:CodeInstitute+DDA101+2021_T4/courseware/468437859a944f7d81a34234957d825b/c8ea2343476c48739676b7f03ba9b08e/) 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ppscore as pps\n",
    "\n",
    "def heatmap_corr(df, threshold, figsize=(10, 8), font_annot=8):\n",
    "    \"\"\"\n",
    "    Heatmap for pearson (linear) and spearman (monotonic) correlations to \n",
    "    visualize only those correlation levels greater than a given threshold.\n",
    "    \"\"\"\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        mask[abs(df) < threshold] = True\n",
    "\n",
    "        fig, axes = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='viridis', annot_kws={'size': font_annot}, ax=axes,\n",
    "                    linewidth=0.01, linecolor='WhiteSmoke'\n",
    "                    )\n",
    "        axes.set_yticklabels(df.columns, rotation=0)\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(10, 8), font_annot=8):\n",
    "    \"\"\"\n",
    "    Heatmap for power predictive score\n",
    "    PPS == 0 means that there is no predictive power\n",
    "    PPS < 0.2 often means that there is some relevant predictive power but it is weak\n",
    "    PPS > 0.2 often means that there is strong predictive power\n",
    "    PPS > 0.8 often means that there is a deterministic relationship in the data,\n",
    "    \"\"\"\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[abs(df) < threshold] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                         mask=mask, cmap='rocket_r', annot_kws={'size': font_annot},\n",
    "                         linewidth=0.01, linecolor='WhiteSmoke')\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def calculate_corr_and_pps(df):\n",
    "    \"\"\"\n",
    "    Calculate the correlations and ppscore of a given dataframe\n",
    "    \"\"\"\n",
    "    df_corr_spearman = df.corr(method='spearman')\n",
    "    df_corr_pearson = df.corr(method='pearson')\n",
    "\n",
    "    pps_matrix_raw = pps.matrix(df)\n",
    "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "    pps_score_stats = pps_matrix_raw.query('ppscore < 1').filter(['ppscore']).describe().T\n",
    "    print('PPS threshold - check PPS score IQR to decide threshold for heatmap \\n')\n",
    "    print(pps_score_stats.round(4))\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "\n",
    "def display_corr_and_pps(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
    "                      figsize=(10, 8), font_annot=8):\n",
    "    \"\"\"\n",
    "    Render the correlations and ppscore heatmaps for a given dataframe\n",
    "    \"\"\"\n",
    "    # print('\\n')\n",
    "    print('To analyze: \\n** Colinearity: how the target variable is correlated with the other features (variables)')\n",
    "    print('** Multi-colinearity: how each feature correlates among themselves (multi-colinearity)')\n",
    "\n",
    "    print('\\n')\n",
    "    print('*** Heatmap: Pearson Correlation ***')\n",
    "    print(f'It evaluates the linear relationship between two continuous variables \\n'\n",
    "          f'* A +ve correlation indicates that as one variable increases the other variable tends to increase.\\n'\n",
    "          f'A correlation near zero indicates that as one variable increases, there is no tendency in the other variable to either increase or decrease.\\n'\n",
    "          f'A -ve correlation indicates that as one variable increases the other variable tends to decrease.')\n",
    "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print('\\n')\n",
    "    print(f'*** Heatmap: Spearman Correlation ***')\n",
    "    print(f'It evaluates monotonic relationship \\n'\n",
    "          f'Spearman correlation coefficients range from -1 to +1.\\n'\n",
    "          f'The sign of the coefficient indicates whether it is a positive or negative monotonic relationship.\\n'\n",
    "          f'* A positive correlation means that as one variable increases, the other variable also tends to increase.')\n",
    "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print('\\n')\n",
    "    print('*** Heatmap: Power Predictive Score (PPS) ***')\n",
    "    print(f'PPS detects linear or non-linear relationships between two columns.\\n'\n",
    "          f'The variable on the x-axis is used to predict the corresponding variable on the y-axis.\\n'\n",
    "          f'The score ranges from 0 (no predictive power) to 1 (perfect predictive power)\\n\\n'\n",
    "          f'* PPS == 0 means that there is no predictive power\\n'\n",
    "          f'* PPS < 0.2 often means that there is some relevant predictive power but it is weak\\n'\n",
    "          f'* PPS > 0.2 often means that there is strong predictive power\\n'\n",
    "          f'* PPS > 0.8 often means that there is a deterministic relationship in the data\\n')\n",
    "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = df_train.drop(['Data_No', 'Flow_rate', 'Dust', 'mass_g', 'Dust_feed', 'Tt', 'bin_size', 'median_DP', 'std_DP', 'change_DP', 'change_EWM'], axis=\"columns\")\n",
    "df_corr_pearson, df_corr_spearman, pps_matrix = calculate_corr_and_pps(df_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_corr_and_pps(df_corr_pearson = df_corr_pearson, df_corr_spearman = df_corr_spearman,\n",
    "                    pps_matrix = pps_matrix, CorrThreshold = 0, PPS_Threshold =0,\n",
    "                    figsize=(12,10), font_annot=10\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Maintaining the observations made to the original data, we also note:\n",
    "\n",
    "#### Heatmap: **Pearson Correlation**\n",
    "* \n",
    "\n",
    "#### Heatmap: **Spearman Correlation**\n",
    "* \n",
    "\n",
    "#### Heatmap: **Power Predictive Score (PPS)**\n",
    "*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Observations\n",
    "#### Heatmap: **Pearson Correlation**\n",
    "* A linear relationship is one when a change in one variable is associated with a proportional change in the other variable \n",
    "* Positive relationships can be observed between \n",
    "    * **Differential Pressure** and **Time** plus **Flow Rate** with a negative \n",
    "    * **Flow Rate** and **RUL** plus **Time** \n",
    "* Strongly negative between **Differential Pressure** and **RUL** \n",
    "\n",
    "#### Heatmap: **Spearman Correlation**\n",
    "* A monotonic relationship is one where one variable is associated with a **change in the specific direction** of another variable. \n",
    "    * e.g. Does a positive change in value/direction X result in a positive change in the value/direction of Y?\n",
    "    * We consider Spearman’s correlation when \n",
    "        * we have pairs of continuous variables and the relationships between them don’t follow a straight line (curvilinear), and/or \n",
    "        * we have pairs of ordinal data (like time)\n",
    "\n",
    "* **Spearman's rho Values and Direction**\n",
    "    * **Differential Pressure** is strongly positively correlated to **Time**, less so **Flow Rate** and negatively correlated to **RUL**\n",
    "    * **Dust Feed** is negatively correlated to **RUL** whereas **Dust Type** is positively correlated to **RUL**\n",
    "    * **Flow Rate** is positively correlated to **Time** and **Differential Pressure** as noted above.\n",
    "\n",
    "#### Heatmap: **Power Predictive Score (PPS)**\n",
    "* Detects linear or non-linear relationships between two columns.\n",
    "* We see strong predictive power between **Dust_feed** and **RUL**, less so however still significant with **Dust_feed** and **Flow_rate**\n",
    "    * RUL as a calculation of **time** remaining, is logically affected by the volume of dust per second. The lower the flow or feed, the higher the RUL. This is however dictated by the simple fact that the filter needs to filter dust. Reducing either of the rates naturally negates the purpose of the filtering process, so we will treat it as a **confounding** relationship and as such, cannot be described in terms of correlations or associations.\n",
    "* When considering the absolute levels of the scores in the dataset, we see a weak yet strong predictive relationship between **Differential Pressure** and **RUL**\n",
    "    * Differential pressure has predictive power of RUL, whereas RUL has no predictive power on differential pressure\n",
    "    * Naturally we also see a week two way relationship between  **Differential Pressure** and **Time** -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Iteration\n",
    "Repeating a set of tasks to achieve a result.\n",
    "\n",
    "* The Model Level: Fitting Parameters\n",
    "* The Micro Level: Tuning Hyperparameters\n",
    "* The Macro Level: Solving Our Problem\n",
    "* The Meta Level: Improving Our Data\n",
    "* The Human Level: Improving Our Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide **df_total** into original **df_test** & **df_train** proportions as supplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_no_total = df_total['Data_No'].map(int).round(decimals=0)\n",
    "# df_total['Data_No'] = data_no_total\n",
    "# n = df_total['Data_No'][0:len(df_total)]\n",
    "# df_train = df_total[n < 51].reset_index(drop=True, names='index')\n",
    "# df_test = df_total[n > 50].reset_index(drop=True, names='index')\n",
    "# del df_train['RUL']\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Datasets \n",
    "Save the files to /transformed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "  os.makedirs(name='outputs/datasets/transformed')\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "df_train.to_csv(f'outputs/datasets/transformed/dfTransformedTrain.csv',index=False)\n",
    "df_test.to_csv(f'outputs/datasets/transformed/dfTransformedTest.csv',index=False)\n",
    "df_total.to_csv(f'outputs/datasets/transformed/dfTransformedTotal.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions: \n",
    "* \n",
    "\n",
    "#### Next Steps:\n",
    "* Regression Model\n",
    "* Classification Model\n",
    "* Cluster Model\n",
    "* Correlation Study"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
